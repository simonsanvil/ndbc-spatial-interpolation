{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training, Hyperparameter Tuning, and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "import pickle\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# modeling/experimentation\n",
    "import mlflow\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# to load parameters of the experiments\n",
    "from kedro.config import ConfigLoader\n",
    "\n",
    "# utils\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed\n",
    "from pandas import IndexSlice as idx\n",
    "from IPython.display import display\n",
    "\n",
    "# viz\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# experiments\n",
    "import experiments.noaa.machine_learning as ml_experiments\n",
    "\n",
    "# local utils and other imports\n",
    "from spatial_interpolation import data, utils\n",
    "from spatial_interpolation.utils.modeling import (\n",
    "    tweak_features,\n",
    "    fit_estimator_with_params,\n",
    "    compute_metrics,\n",
    ")\n",
    "from spatial_interpolation.utils.experiments import conf \n",
    "from spatial_interpolation.data.load_data import get_ml_workspace\n",
    "from spatial_interpolation.utils import tqdm_joblib\n",
    "\n",
    "# configuration\n",
    "import dotenv\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track the experiment with MLFlow\n",
    "# ws = data.get_ml_workspace(auth=\"service_principal\")\n",
    "# tracking_uri = ws.get_mlflow_tracking_uri()\n",
    "# mlflow.set_tracking_uri(tracking_uri)\n",
    "# mlflow.set_experiment(os.environ.get(\"MLFLOW_EXPERIMENT_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load features\n",
    "config_name = \"rf_config_set2\"\n",
    "experiment = ml_experiments.NOAAMLTraining(config_name)#, mlflow_tracking_uri=tracking_uri)\n",
    "config = experiment.get_config()\n",
    "\n",
    "train_df = pd.concat(\n",
    "    [pd.read_parquet(f\"{config.input.train_dir}/{year}.parquet\") for year in range(2011,2022)],\n",
    "    axis=0).sort_index()\n",
    "test_df = pd.concat(\n",
    "    [pd.read_parquet(f\"{config.input.eval_dir}/{year}.parquet\") for year in range(2011,2022)],\n",
    "    axis=0).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the X and Y train and test sets that will be passed to the model\n",
    "X_train = train_df.drop(columns=[config.target]).copy()\n",
    "y_train = train_df[config.target]\n",
    "X_eval = test_df.drop(columns=[config.target]).copy()\n",
    "y_eval = test_df[config.target]\n",
    "\n",
    "X_train, X_eval = tweak_features(\n",
    "    config.pretrain_funcs,\n",
    "    X_train, X_eval\n",
    ")\n",
    "y_train = y_train.loc[X_train.index]\n",
    "y_eval = y_eval.loc[X_eval.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_configs = ml_experiments.NOAAMLTraining.config.get_config()\n",
    "experiment_name = ml_experiments.NOAAMLTraining.__name__\n",
    "for config_name in available_configs:\n",
    "    if \"lgbm\" in config_name:\n",
    "        continue\n",
    "    p = subprocess.Popen([\"python\", \"-m\", \"experiments\", experiment_name , config_name])\n",
    "    print(f\"Started experiment {config_name} on process {p.pid}\")\n",
    "p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = \"experiment2\"\n",
    "experiment = ml_experiments.NOAAMLTraining(config_name, mlflow_tracking_uri=None)\n",
    "preds_df = experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = \"experiment3\"\n",
    "experiment = ml_experiments.NOAAMLTraining(config_name, mlflow_tracking_uri=None)\n",
    "preds_df = experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subplotted import subplotted\n",
    "locations = y_eval.index.get_level_values(\"location_id\").unique()\n",
    "for S, ax, location in subplotted(locations, ncols=3, figsize=(20,5)):\n",
    "    sns.lineplot(data=y_eval.loc[location].reset_index(), x=\"time\", y=y_eval.name, ax=ax, label=location)\n",
    "    ax.set_title(location)\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "else:\n",
    "    S.fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "We'll [Grid Search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) \n",
    "and [Random Search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) on a subset of the data to tune some of the hyperparameters of our model(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Experiment Configuration, search and evaluate hyperparameters\n",
    "config_name = \"rf_config_set1\"\n",
    "experiment = ml_experiments.NOAAMLTraining(config_name)#, mlflow_tracking_uri=tracking_uri)\n",
    "config = experiment.get_config()\n",
    "target = config.target\n",
    "\n",
    "mod = config.model()\n",
    "fit_params = config.fit_params.to_dict()\n",
    "if fit_params.get(\"eval_set\"):\n",
    "    fit_params[\"eval_set\"] = [(X_eval, y_eval)]\n",
    "search_params = config[\"param_search\"].get(\"parameters_to_search\", {}).to_dict()\n",
    "\n",
    "for param in config[\"model_params\"]:\n",
    "    if param not in search_params:\n",
    "        search_params[param] = [config[\"model_params\"][param]]\n",
    "\n",
    "parameters_to_search = list(ParameterGrid(search_params))\n",
    "\n",
    "if config.param_search.strategy == \"random\":\n",
    "    assert \"size\" in config.param_search, \"size must be specified for random search\"\n",
    "    n_params = min(config.param_search.size, len(parameters_to_search))\n",
    "    print(f\"{n_params}/{len(parameters_to_search)} random search iterations  will be done\")\n",
    "    # choose a random subset of the parameters to search\n",
    "    parameters_to_search = np.random.choice(parameters_to_search, size=n_params, replace=False)\n",
    "\n",
    "# with tqdm_joblib(tqdm(desc=\"training grid...\",total=len(parameters_to_search))) as pbar:\n",
    "#     results = Parallel(n_jobs=30)(\n",
    "#         delayed(fit_estimator_with_params)(mod,X=X_train,y=y_train.values,X_eval=X_eval,y_eval=y_eval.values,params=params,fit_params=fit_params)\n",
    "#         for params in parameters_to_search\n",
    "#     )\n",
    "\n",
    "results = [\n",
    "    fit_estimator_with_params(mod,X=X_train,y=y_train.values,X_eval=X_eval,y_eval=y_eval.values,params=params,fit_params=fit_params)\n",
    "    for params in tqdm(parameters_to_search)\n",
    "]\n",
    "\n",
    "\n",
    "# # get best estimator and results\n",
    "best_estimator = min(results,key=lambda d: d[\"est_best_score\"])[\"estimator\"]\n",
    "print(\"Best results:\");pprint(min(results,key=lambda d: d[\"est_best_score\"]))\n",
    "print(\"Worst results:\");pprint(max(results,key=lambda d: d[\"est_best_score\"]))\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"est_best_score\",ascending=True)\n",
    "results_df = pd.concat([results_df,results_df.params.apply(pd.Series)],axis=1)\n",
    "results_df.index.name = \"round\"\n",
    "\n",
    "print(results_df.iloc[0][\"params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = config.model(n_jobs=-1, n_estimators=600)\n",
    "mod.fit(X_train, y_train.values)\n",
    "preds = mod.predict(X_eval)\n",
    "r2_score(y_eval, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/06_models/rf_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mod, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_eval, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_eval, preds) # n_estimators = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "config_name = \"rf_config\"\n",
    "experiment = ml_experiments.NOAAMLTraining(config_name)#, mlflow_tracking_uri=tracking_uri)\n",
    "config = experiment.get_config() \n",
    "target = config.target\n",
    "mod = config.model()\n",
    "\n",
    "parameters_to_search = config.parameters_to_search.to_dict() \n",
    "random = RandomizedSearchCV(estimator = mod, param_distributions = parameters_to_search, n_iter = 80, cv = 2, verbose=2, random_state=42, n_jobs = 35)\n",
    "random.fit(X_train, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show param progression\n",
    "sns.lineplot(x=\"learning_rate\",y=\"est_best_score\",data=results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show metrics of best estimator by evaluation set (train, eval)\n",
    "metrics = {}\n",
    "for split,y,X in zip([\"train\",\"eval\"],[y_train,y_eval],[X_train,X_eval]):\n",
    "    if len(X)==0: continue\n",
    "    pred = best_estimator.predict(X)\n",
    "    split_metrics = compute_metrics(y,pred)\n",
    "    split_metrics_dict = {\n",
    "        f\"{k}_{split}\":[v]\n",
    "        for k,v in split_metrics.to_dict().items()\n",
    "    }\n",
    "    metrics.update(split_metrics_dict)\n",
    "metrics_df = pd.DataFrame(metrics,index=[\"score\"]).rename_axis(\"metric\").T\n",
    "display(metrics_df.sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show metrics by buoy\n",
    "preds = []\n",
    "for split,y,X in zip([\"train\",\"eval\"],[y_train,y_eval],[X_train,X_eval]):\n",
    "    if len(X)==0: continue\n",
    "    pred = best_estimator.predict(X)\n",
    "    split_metrics = compute_metrics(y.values[:,0],pred)\n",
    "    split_metrics_dict = {\n",
    "        f\"{k}_{split}\":[v]\n",
    "        for k,v in split_metrics.to_dict().items()\n",
    "    }\n",
    "    metrics.update(split_metrics_dict)\n",
    "    y_preds = pd.DataFrame(dict(true=y.values[:,0],pred=pred),index=X.index)\n",
    "    y_preds[\"split\"] = split\n",
    "    preds.append(y_preds)\n",
    "preds_df = pd.concat(preds,axis=0)\n",
    "preds_df.groupby(\"buoy_id\").apply(\n",
    "    lambda g: compute_metrics(g.true,g.pred)\n",
    ").assign(is_eval=lambda df: df.index.isin(y_eval.index.get_level_values(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log metrics\n",
    "# mlflow.log_metrics({f\"best__{row.name}\":round(row.score,4) for row in metrics_df.iloc()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log metrics figure\n",
    "ax = sns.barplot(data=metrics_df.T.round(3))\n",
    "ax.set(title=f\"Best metrics\")\n",
    "ax.bar_label(ax.containers[0])\n",
    "fig = ax.get_figure();\n",
    "# mlflow.log_figure(fig,f\"best_metrics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log the shap values\n",
    "shap_values = shap.TreeExplainer(best_estimator).shap_values(X_eval)\n",
    "fig, ax = plt.subplots(figsize=(18,15),facecolor=\"white\")\n",
    "shap.summary_plot(shap_values, X_eval)\n",
    "plt.tight_layout(); fig.set_figwidth(18)\n",
    "# mlflow.log_figure(fig,f\"shap_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model with the best parameter on all data\n",
    "X = pd.concat([X_train,X_eval],axis=0)\n",
    "y = pd.concat([y_train,y_eval],axis=0)\n",
    "mod.set_params(**best_estimator.get_params()) # parameters[\"model_params\"]\n",
    "mod.fit(X,y,**fit_params)\n",
    "mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end experiment run\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5156605a370da4264d35700d255afe940b4002408cb66ca14decdceceba1912f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
